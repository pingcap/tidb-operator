apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: "local-storage"
provisioner: "kubernetes.io/no-provisioner"
volumeBindingMode: "WaitForFirstConsumer"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: kube-system
data:
  storageClassMap: |
    local-storage:
      hostDir: /mnt/disks
      mountDir: /mnt/disks

---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: kube-system
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      hostPID: true
      nodeSelector:
        cloud.google.com/gke-os-distribution: ubuntu
        cloud.google.com/gke-local-ssd: "true"
      serviceAccountName: local-storage-admin
      initContainers:
        - name: local-ssd-startup
          image: alpine
          command: ['/bin/sh', '-c', 'nsenter -t 1 -m -u -i -n -p -- bash -c "${STARTUP_SCRIPT}"']
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /mnt/disks
            name: local-disks
            mountPropagation: Bidirectional
          env:
          - name: STARTUP_SCRIPT
            value: |
                #!/usr/bin/env bash
                set -euo pipefail
                set -x

                # Install the linux guest environment tools
                export DEBIAN_FRONTEND=noninteractive
                cat /etc/apt/sources.list
                dpkg --configure -a
                apt-get update
                apt-get install -y software-properties-common || echo "could not install software-properties-common"
                apt-add-repository universe
                apt-get update
                declare -a PKG_LIST=(python-google-compute-engine \
                python3-google-compute-engine \
                google-compute-engine-oslogin \
                gce-compute-image-packages)
                for pkg in ${PKG_LIST[@]}; do
                  apt-get install -y $pkg || echo "Not available: $pkg"
                done

                apt-get install -y lvm2
                apt-get -y autoremove

                set -x
                if ! findmnt -n -a -l | grep /mnt/disks/ssd ; then
                  if test -f /etc/ssd_mounts ; then
                    ssd_mounts=$(cat /etc/ssd_mounts)
                  else
                    echo "no ssds mounted yet"
                    exit 1
                  fi
                else
                  ssd_mounts=$(findmnt -n -a -l --nofsroot | grep /mnt/disks/ssd)
                  echo "$ssd_mounts" > /etc/ssd_mounts
                fi

                # Re-mount all disks as a single logical volume
                for ssd in $(findmnt -n -a -l --nofsroot | grep /mnt/disks/ssd | awk '{print $1}') ; do
                  umount "$ssd"
                done
                for ssd in $(echo "$ssd_mounts" | awk '{print $1}') ; do
                  if test -d "$ssd"; then
                    rm -r "$ssd"
                  fi
                done

                if ! pvs | grep volume_all_ssds ; then
                  for dev in $(echo "$ssd_mounts" | awk '{print $2}') ; do
                    wipefs --all "$dev"
                  done
                  echo "$ssd_mounts" | awk '{print $2}' | xargs /sbin/pvcreate
                fi
                pvdisplay
                pvs
                if ! vgs | grep volume_all_ssds ; then
                  echo "$ssd_mounts" | awk '{print $2}' | xargs /sbin/vgcreate volume_all_ssds
                fi
                vgdisplay
                vgs
                if ! lvs | grep logical_all_ssds ; then
                  lvcreate -l 100%FREE -n logical_all_ssds volume_all_ssds
                fi
                lvdisplay
                lvs

                if ! uuid=$(blkid -s UUID -o value /dev/volume_all_ssds/logical_all_ssds) ; then
                  mkfs.ext4 /dev/volume_all_ssds/logical_all_ssds
                  uuid=$(blkid -s UUID -o value /dev/volume_all_ssds/logical_all_ssds)
                fi

                mnt_dir="/mnt/disks/$uuid"
                mkdir -p "$mnt_dir"

                if ! grep "$uuid" /etc/fstab ; then
                  new_fstab=$(grep -v /mnt/disks/ssd /etc/fstab)
                  echo "$new_fstab" > /etc/fstab
                  echo "UUID=$uuid $mnt_dir ext4 rw,relatime,discard,nobarrier,data=ordered" >> /etc/fstab
                fi
                mount -a
      containers:
        - image: "quay.io/external_storage/local-volume-provisioner:v2.2.0"
          name: provisioner
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 100m
              memory: 100Mi
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: JOB_CONTAINER_IMAGE
            value: "quay.io/external_storage/local-volume-provisioner:v2.2.0"
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            # mounting /dev in DinD environment would fail
            # - mountPath: /dev
            #   name: provisioner-dev
            - mountPath: /mnt/disks
              name: local-disks
              mountPropagation: "HostToContainer"
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        # - name: provisioner-dev
        #   hostPath:
        #     path: /dev
        - name: local-disks
          hostPath:
            path: /mnt/disks

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io
