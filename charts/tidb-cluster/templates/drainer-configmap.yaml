{{- if .Values.drainer.create }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.clusterName }}-drainer
  labels:
    app.kubernetes.io/name: {{ template "tidb-cluster.name" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Values.clusterName }}
    app.kubernetes.io/component: drainer
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+"  "_" }}
data:
  drainer-config: |-
    # drainer Configuration.

    # addr (i.e. 'host:port') to listen on for drainer connections
    # will register this addr into etcd
    # addr = "127.0.0.1:8249"

    # the interval time (in seconds) of detect pumps' status
    detect-interval = 10

    # drainer meta data directory path
    data-dir = "data.drainer"

    # a comma separated list of PD endpoints
    pd-urls = "http://127.0.0.1:2379"

    #[security]
    # Path of file that contains list of trusted SSL CAs for connection with cluster components.
    # ssl-ca = "/path/to/ca.pem"
    # Path of file that contains X509 certificate in PEM format for connection with cluster components.
    # ssl-cert = "/path/to/pump.pem"
    # Path of file that contains X509 key in PEM format for connection with cluster components.
    # ssl-key = "/path/to/pump-key.pem"

    # syncer Configuration.
    [syncer]

    # disable sync these schema
    ignore-schemas = "INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql"

    # number of binlog events in a transaction batch
    txn-batch = 1

    # work count to execute binlogs
    worker-count = 1

    disable-dispatch = false

    # safe mode will split update to delete and insert
    safe-mode = false

    # downstream storage, equal to --dest-db-type
    # valid values are "mysql", "pb", "tidb", "flash", "kafka"
    db-type = "{{ .Values.drainer.destDBType }}"

    ##replicate-do-db priority over replicate-do-table if have same db name
    ##and we support regex expression , start with '~' declare use regex expression.
    #
    #replicate-do-db = ["~^b.*","s1"]
    #[[syncer.replicate-do-table]]
    #db-name ="test"
    #tbl-name = "log"

    #[[syncer.replicate-do-table]]
    #db-name ="test"
    #tbl-name = "~^a.*"

{{- if eq .Values.drainer.destDBType "mysql" }}
    # the downstream mysql protocol database
    [syncer.to]
    host = "{{ .Values.drainer.mysql.host }}"
    user = "{{ .Values.drainer.mysql.user }}"
    password = "{{ .Values.drainer.mysql.password }}"
    port = {{ .Values.drainer.mysql.port }}
    # Time and size limits for flash batch write
    time-limit = "{{ .Values.drainer.mysql.timeLimit | default "30s" }}"
    size-limit = "{{ .Values.drainer.mysql.sizeLimit | default "100000" }}"
{{- end }}

{{- if eq .Values.drainer.destDBType "pb" }}
    # Uncomment this if you want to use pb or sql as db-type.
    # Compress compresses output file, like pb and sql file. Now it supports "gzip" algorithm only.
    # Values can be "gzip". Leave it empty to disable compression.
    [syncer.to]
    dir = "/data/pb"
    compression = "gzip"
{{- end }}


{{- if eq .Values.drainer.destDBType "kafka" }}
    # when db-type is kafka, you can uncomment this to config the down stream kafka, it will be the globle config kafka default
    [syncer.to]
    # only need config one of zookeeper-addrs and kafka-addrs, will get kafka address if zookeeper-addrs is configed.
    {{- if .Values.drainer.kafka.zookeeperAddrs }}
    zookeeper-addrs = {{ .Values.drainer.kafka.zookeeperAddrs }}
    {{- end }}
    {{- if .Values.drainer.kafka.kafkaAddrs }}
    kafka-addrs = {{ .Values.drainer.kafka.kafkaAddrs }}
    {{- end }}
    kafka-version = {{ .Values.drainer.kafka.kafkaVersion | default "0.8.2.0" }}
{{- end }}
{{- end }}
