apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.clusterName }}-pd
  labels:
    app: {{ template "tidb-cluster.name" . }}
    component: pd
    chart: {{ .Chart.Name }}-{{ .Chart.Version }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  startup-script: |-
    #!/bin/sh

    # This script is used to start pd containers in kubernetes cluster

    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #

    set -euo pipefail
    ANNOTATIONS="/etc/podinfo/annotations"
    # get statefulset ordinal index for current pod
    ORDINAL=$(echo ${HOSTNAME} | awk -F '-' '{print $NF}')

    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null || true

    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
    	echo "entering debug mode."
    	tail -f /dev/null
    fi

    ARGS="--data-dir=/var/lib/pd \
    --name=${HOSTNAME} \
    --peer-urls=http://0.0.0.0:2380 \
    --advertise-peer-urls=http://${HOSTNAME}.${PEER_SERVICE_NAME}.${NAMESPACE}.svc:2380 \
    --client-urls=http://0.0.0.0:2379 \
    --advertise-client-urls=http://${HOSTNAME}.${PEER_SERVICE_NAME}.${NAMESPACE}.svc:2379 \
    --config=/etc/pd/pd.toml \
    "

    if [[ ${ORDINAL} -lt ${INITIAL_REPLICAS} ]]
    then
        INITIALS="${SET_NAME}-0=http://${SET_NAME}-0.${PEER_SERVICE_NAME}.${NAMESPACE}.svc:2380"
        TOP=$((INITIAL_REPLICAS-1))
        for i in $(seq 1 ${TOP});
        do
            INITIALS="${INITIALS},${SET_NAME}-${i}=http://${SET_NAME}-${i}.${PEER_SERVICE_NAME}.${NAMESPACE}.svc:2380"
        done
        ARGS="${ARGS}--initial-cluster=${INITIALS}"
    else
        EXISTS="http://${SET_NAME}-0.${PEER_SERVICE_NAME}.${NAMESPACE}.svc:2380"
        TOP=$((ORDINAL-1))
        for i in $(seq 1 ${TOP});
        do
            EXISTS="${EXISTS},http://${SET_NAME}-${i}.${PEER_SERVICE_NAME}.${NAMESPACE}.svc:2380"
        done
        ARGS="${ARGS}--join=${EXISTS}"
    fi

    echo "starting pd-server ..."
    echo "/pd-server ${ARGS}"
    exec /pd-server ${ARGS}
  config-file: |-
    # PD Configuration.

    name = "pd"
    data-dir = "default.pd"

    client-urls = "http://127.0.0.1:2379"
    # if not set, use ${client-urls}
    advertise-client-urls = ""

    peer-urls = "http://127.0.0.1:2380"
    # if not set, use ${peer-urls}
    advertise-peer-urls = ""

    initial-cluster = ""
    initial-cluster-state = ""

    lease = 3
    tso-save-interval = "3s"

    [log]
    level = "{{ .Values.pd.logLevel }}"

    # log format, one of json, text, console
    #format = "text"

    # disable automatic timestamps in output
    #disable-timestamp = false

    # file logging
    [log.file]
    #filename = ""
    # max log file size in MB
    #max-size = 300
    # max log file keep days
    #max-days = 28
    # maximum number of old log files to retain
    #max-backups = 7
    # rotate log by day
    #log-rotate = true

    [metric]
    # prometheus client push interval, set "0s" to disable prometheus.
    interval = "15s"
    # prometheus pushgateway address, leaves it empty will disable prometheus.
    address = ""

    [schedule]
    max-snapshot-count = 3
    max-store-down-time = "{{ .Values.pd.maxStoreDownTime }}"
    leader-schedule-limit = 64
    region-schedule-limit = 16
    replica-schedule-limit = 24

    # customized schedulers, the format is as below
    # if empty, it will use balance-leader, balance-region, hot-region as default
    # [[schedule.schedulers]]
    # type = "evict-leader"
    # args = ["1"]


    [replication]
    # The number of replicas for each region.
    max-replicas = {{ .Values.pd.maxReplicas }}
    # The label keys specified the location of a store.
    # The placement priorities is implied by the order of label keys.
    # For example, ["zone", "rack"] means that we should place replicas to
    # different zones first, then to different racks if we don't have enough zones.
    location-labels = ["zone", "rack", "host"]
