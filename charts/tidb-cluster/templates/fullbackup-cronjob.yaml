{{- if .Values.fullbackup.create }}
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Values.clusterName }}-fullbackup
  labels:
    app.kubernetes.io/name: {{ template "tidb-cluster.name" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Values.clusterName }}
    app.kubernetes.io/component: fullbackup
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+"  "_" }}
spec:
  schedule: "{{ .Values.fullbackup.schedule }}"
  concurrencyPolicy: Forbid
  suspend: {{ .Values.fullbackup.suspend }}
  startingDeadlineSeconds: {{ .Values.fullbackup.startingDeadlineSeconds }}
  jobTemplate:
    spec:
      template:
        spec:
          initContainers:
          - name: get-ts
            image: {{ .Values.fullbackup.binlogImage }}
            command:
            - /binlogctl
            - -pd-urls=http://{{ .Values.clusterName }}-pd:2379
            - -cmd=generate_meta
            - -data-dir=/savepoint-dir
            volumeMounts:
            - name: savepoint-dir
              mountPath: "/savepoint-dir"
          containers:
          - name: fullbackup
            image: pingcap/tidb-enterprise-tools:latest
            command:
            - /bin/sh
            - -c
            - |
              set -euo pipefail
              dirname=`date +%Y-%m-%dT%H%M%S`-${MY_POD_NAME}
              host=`echo {{ .Values.clusterName }}_TIDB_SERVICE_HOST | tr '[a-z]' '[A-Z]'`

              mkdir -p /data/${dirname}/
              cp /savepoint-dir/savepoint /data/${dirname}/

              /mydumper \
                --outputdir=/data/${dirname} \
                --host=`eval echo '${'$host'}'` \
                --port=4000 \
                --user={{ .Values.fullbackup.user }} \
                --password={{ .Values.fullbackup.password }} \
                {{ .Values.fullbackup.options }}

              cd /data; ls -t | sed '1,{{ .Values.fullbackup.retentionCount }}d' | xargs -i -n1 rm -r {}
            volumeMounts:
            - name: savepoint-dir
              mountPath: "/savepoint-dir"
            - name: data
              mountPath: "/data"
            ENV:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          restartPolicy: OnFailure
          volumes:
          - name: savepoint-dir
            emptyDir: {}
          - name: data
            persistentVolumeClaim:
              claimName: {{ .Values.clusterName }}-fullbackup
{{- end }}
